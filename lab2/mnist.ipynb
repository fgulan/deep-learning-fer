{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import skimage as ski\n",
    "import skimage.io\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "DATA_DIR = './mnist/'\n",
    "SAVE_DIR = \"./out_mnist/\"\n",
    "WEIGHT_DECAY = 1e-4\n",
    "MAX_EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "LR_POLICY = {1:{'lr':1e-1}, 3:{'lr':1e-2}, 5:{'lr':1e-3}, 7:{'lr':1e-4}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_dataset(data_dir):    \n",
    "    dataset = input_data.read_data_sets(data_dir, one_hot=True)\n",
    "    train_x = dataset.train.images\n",
    "    train_x = train_x.reshape([-1, 1, 28, 28])\n",
    "    train_y = dataset.train.labels\n",
    "    valid_x = dataset.validation.images\n",
    "    valid_x = valid_x.reshape([-1, 1, 28, 28])\n",
    "    valid_y = dataset.validation.labels\n",
    "    test_x = dataset.test.images\n",
    "    test_x = test_x.reshape([-1, 1, 28, 28])\n",
    "    test_y = dataset.test.labels\n",
    "    train_mean = train_x.mean()\n",
    "    train_x -= train_mean\n",
    "    valid_x -= train_mean\n",
    "    test_x -= train_mean\n",
    "    return train_x, train_y, valid_x, valid_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(X, Y_, batch_size):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    for start_idx in range(0, X.shape[0] - batch_size + 1, batch_size):\n",
    "        indices_range = indices[start_idx:start_idx + batch_size]\n",
    "        yield X[indices_range], Y_[indices_range]\n",
    "\n",
    "def get_trainable_weights():\n",
    "    weights = []\n",
    "    for var in tf.trainable_variables():\n",
    "        if \"/kernel:0\" not in var.name: \n",
    "            continue\n",
    "        weights.append(var)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_conv_filters(session, layer, epoch, step, name, save_dir):\n",
    "    weights = session.run(layer).copy()\n",
    "    num_filters = weights.shape[3]\n",
    "    num_channels = weights.shape[2]\n",
    "    k = weights.shape[0]\n",
    "    assert weights.shape[0] == weights.shape[1]\n",
    "    weights -= weights.min()\n",
    "    weights /= weights.max()\n",
    "    border = 1\n",
    "    cols = 8\n",
    "    rows = math.ceil(num_filters / cols)\n",
    "    width = cols * k + (cols-1) * border\n",
    "    height = rows * k + (rows-1) * border\n",
    "    img = np.zeros([height, width, num_channels])\n",
    "    for i in range(num_filters):\n",
    "        r = int(i / cols) * (k + border)\n",
    "        c = int(i % cols) * (k + border)\n",
    "        img[r:r+k,c:c+k,:] = weights[:,:,:,i]\n",
    "        \n",
    "    img = img.reshape(height, width)\n",
    "    filename = '%s_epoch_%02d_step_%06d.png' % (name, epoch, step)\n",
    "    ski.io.imsave(os.path.join(save_dir, filename), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(inputs, filters=32, kernel_size=[5, 5], \n",
    "               activation=tf.nn.relu, regularizer=tf.contrib.layers.l2_regularizer(scale=WEIGHT_DECAY), name=None):\n",
    "    return tf.layers.conv2d(inputs, filters, kernel_size, padding='same', \n",
    "                            activation=activation, kernel_regularizer=regularizer, \n",
    "                            kernel_initializer=tf.variance_scaling_initializer(), name=name)\n",
    "\n",
    "def max_pool_layer(inputs, pool_size=[2, 2], strides=2, name=None):\n",
    "    return tf.layers.max_pooling2d(inputs, pool_size, strides, padding='same', name=name)\n",
    "\n",
    "def fc_layer(input, units, activation=tf.nn.relu, regularizer=tf.contrib.layers.l2_regularizer(scale=WEIGHT_DECAY), name=None):\n",
    "    return tf.layers.dense(input, units, activation, kernel_regularizer=regularizer, \n",
    "                           kernel_initializer=tf.variance_scaling_initializer(), name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dnn(inputs):\n",
    "    input_layer = tf.reshape(inputs, [-1, 28, 28, 1])\n",
    "    conv1 = conv_layer(input_layer, 16, name=\"conv1\")\n",
    "    pool1 = max_pool_layer(conv1, name=\"pool1\")\n",
    "\n",
    "    conv2 = conv_layer(pool1, 32, name=\"conv2\")\n",
    "    pool2 = max_pool_layer(conv2, name=\"pool2\")\n",
    "    \n",
    "    flat_pool2 = tf.contrib.layers.flatten(pool2)\n",
    "\n",
    "    fc1 = fc_layer(flat_pool2, 512, name=\"fc1\")\n",
    "    logits = fc_layer(fc1, 10, activation=None, regularizer=None, name=\"logits\")\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train_x, train_y, valid_x, valid_y, test_x, test_y = get_mnist_dataset(DATA_DIR)\n",
    "x = tf.placeholder(tf.float32, [None, 1, 28, 28])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "y_conv = build_dnn(x)\n",
    "weights = get_trainable_weights()[:-1]\n",
    "err_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "reg_loss = sum(map(lambda w : tf.nn.l2_loss(w), weights))\n",
    "loss = err_loss + WEIGHT_DECAY * reg_loss\n",
    "lr = tf.placeholder(tf.float32)\n",
    "train_step = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "num_examples = train_x.shape[0]\n",
    "num_batches = num_examples // BATCH_SIZE\n",
    "print_step = 5\n",
    "draw_step = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 5/429, batch loss 2.15957\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gulan_filip\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\util\\dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 425/429, batch loss 0.0781361\n",
      "epoch 1, valid loss 0.111153, valid accuracy 0.9714\n",
      "epoch 2, step 425/429, batch loss 0.0405968\n",
      "epoch 2, valid loss 0.0767076, valid accuracy 0.984\n",
      "epoch 3, step 425/429, batch loss 0.0594661\n",
      "epoch 3, valid loss 0.0714623, valid accuracy 0.9866\n",
      "epoch 4, step 425/429, batch loss 0.0665175\n",
      "epoch 4, valid loss 0.0706258, valid accuracy 0.9866\n",
      "epoch 5, step 425/429, batch loss 0.0938216\n",
      "epoch 5, valid loss 0.0698938, valid accuracy 0.9878\n",
      "epoch 6, step 425/429, batch loss 0.0589987\n",
      "epoch 6, valid loss 0.069708, valid accuracy 0.9878\n",
      "epoch 7, step 425/429, batch loss 0.0432072\n",
      "epoch 7, valid loss 0.0696822, valid accuracy 0.9878\n",
      "epoch 8, step 425/429, batch loss 0.0967827\n",
      "epoch 8, valid loss 0.0696584, valid accuracy 0.9876\n",
      "epoch 9, step 425/429, batch loss 0.0690163\n",
      "epoch 9, valid loss 0.0696345, valid accuracy 0.9876\n",
      "epoch 10, step 425/429, batch loss 0.0577876\n",
      "epoch 10, valid loss 0.0696141, valid accuracy 0.9876\n",
      "epoch 11, step 425/429, batch loss 0.0302796\n",
      "epoch 11, valid loss 0.0695945, valid accuracy 0.9876\n",
      "epoch 12, step 425/429, batch loss 0.0453346\n",
      "epoch 12, valid loss 0.0695762, valid accuracy 0.9876\n",
      "epoch 13, step 425/429, batch loss 0.0339273\n",
      "epoch 13, valid loss 0.0695593, valid accuracy 0.9876\n",
      "epoch 14, step 425/429, batch loss 0.0381121\n",
      "epoch 14, valid loss 0.0695421, valid accuracy 0.9876\n",
      "epoch 15, step 425/429, batch loss 0.0650434\n",
      "epoch 15, valid loss 0.0695251, valid accuracy 0.9876\n",
      "epoch 16, step 425/429, batch loss 0.0834851\n",
      "epoch 16, valid loss 0.0695079, valid accuracy 0.9876\n",
      "epoch 17, step 425/429, batch loss 0.0741412\n",
      "epoch 17, valid loss 0.0694927, valid accuracy 0.9876\n",
      "epoch 18, step 425/429, batch loss 0.0618655\n",
      "epoch 18, valid loss 0.0694765, valid accuracy 0.9876\n",
      "epoch 19, step 425/429, batch loss 0.0729005\n",
      "epoch 19, valid loss 0.0694586, valid accuracy 0.9876\n",
      "epoch 20, step 425/429, batch loss 0.0394809\n",
      "epoch 20, valid loss 0.0694444, valid accuracy 0.9876\n",
      "epoch 21, step 425/429, batch loss 0.0644657\n",
      "epoch 21, valid loss 0.0694303, valid accuracy 0.9876\n",
      "epoch 22, step 425/429, batch loss 0.0473913\n",
      "epoch 22, valid loss 0.0694167, valid accuracy 0.9876\n",
      "epoch 23, step 425/429, batch loss 0.0651331\n",
      "epoch 23, valid loss 0.0694033, valid accuracy 0.9876\n",
      "epoch 24, step 425/429, batch loss 0.0707063\n",
      "epoch 24, valid loss 0.0693891, valid accuracy 0.9876\n",
      "epoch 25, step 425/429, batch loss 0.0728988\n",
      "epoch 25, valid loss 0.0693742, valid accuracy 0.9876\n",
      "epoch 26, step 425/429, batch loss 0.0523718\n",
      "epoch 26, valid loss 0.0693634, valid accuracy 0.9876\n",
      "epoch 27, step 425/429, batch loss 0.0364067\n",
      "epoch 27, valid loss 0.0693486, valid accuracy 0.9876\n",
      "epoch 28, step 425/429, batch loss 0.1167551\n",
      "epoch 28, valid loss 0.0693376, valid accuracy 0.9876\n",
      "epoch 29, step 425/429, batch loss 0.0331776\n",
      "epoch 29, valid loss 0.0693254, valid accuracy 0.9876\n",
      "epoch 30, step 425/429, batch loss 0.0299754\n",
      "epoch 30, valid loss 0.0693082, valid accuracy 0.9876\n",
      "epoch 31, step 425/429, batch loss 0.0633118\n",
      "epoch 31, valid loss 0.0692935, valid accuracy 0.9876\n",
      "epoch 32, step 425/429, batch loss 0.0552902\n",
      "epoch 32, valid loss 0.0692824, valid accuracy 0.9876\n",
      "epoch 33, step 425/429, batch loss 0.0850909\n",
      "epoch 33, valid loss 0.069269, valid accuracy 0.9876\n",
      "epoch 34, step 425/429, batch loss 0.0462328\n",
      "epoch 34, valid loss 0.0692568, valid accuracy 0.9876\n",
      "epoch 35, step 425/429, batch loss 0.0530596\n",
      "epoch 35, valid loss 0.0692443, valid accuracy 0.9876\n",
      "epoch 36, step 425/429, batch loss 0.0425609\n",
      "epoch 36, valid loss 0.0692302, valid accuracy 0.9876\n",
      "epoch 37, step 425/429, batch loss 0.0875934\n",
      "epoch 37, valid loss 0.0692189, valid accuracy 0.9878\n",
      "epoch 38, step 425/429, batch loss 0.1378755\n",
      "epoch 38, valid loss 0.0692085, valid accuracy 0.9878\n",
      "epoch 39, step 425/429, batch loss 0.0518902\n",
      "epoch 39, valid loss 0.0691956, valid accuracy 0.9878\n",
      "epoch 40, step 425/429, batch loss 0.0394782\n",
      "epoch 40, valid loss 0.0691824, valid accuracy 0.9878\n",
      "epoch 41, step 425/429, batch loss 0.0417924\n",
      "epoch 41, valid loss 0.0691701, valid accuracy 0.9878\n",
      "epoch 42, step 425/429, batch loss 0.0916284\n",
      "epoch 42, valid loss 0.0691593, valid accuracy 0.9878\n",
      "epoch 43, step 425/429, batch loss 0.0566579\n",
      "epoch 43, valid loss 0.0691475, valid accuracy 0.9878\n",
      "epoch 44, step 425/429, batch loss 0.0615426\n",
      "epoch 44, valid loss 0.0691353, valid accuracy 0.9878\n",
      "epoch 45, step 425/429, batch loss 0.0612522\n",
      "epoch 45, valid loss 0.069123, valid accuracy 0.9878\n",
      "epoch 46, step 425/429, batch loss 0.0623483\n",
      "epoch 46, valid loss 0.0691105, valid accuracy 0.9878\n",
      "epoch 47, step 425/429, batch loss 0.0364032\n",
      "epoch 47, valid loss 0.0690985, valid accuracy 0.9878\n",
      "epoch 48, step 425/429, batch loss 0.0608842\n",
      "epoch 48, valid loss 0.0690846, valid accuracy 0.9878\n",
      "epoch 49, step 425/429, batch loss 0.0535565\n",
      "epoch 49, valid loss 0.0690745, valid accuracy 0.9878\n",
      "epoch 50, step 425/429, batch loss 0.0499309\n",
      "epoch 50, valid loss 0.0690612, valid accuracy 0.9878\n",
      "test loss 0.062295, test accuracy 0.9863\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        if epoch in LR_POLICY:\n",
    "            lr_policy = LR_POLICY[epoch]['lr']\n",
    "        for (j, batch) in enumerate(iterate_minibatches(train_x, train_y, BATCH_SIZE)):\n",
    "            train_step.run(feed_dict={x: batch[0], y_: batch[1], lr: lr_policy})\n",
    "            if j % print_step == 0:\n",
    "                batch_loss = loss.eval(feed_dict={x: batch[0], y_: batch[1]})\n",
    "                print('epoch %d, step %d/%d, batch loss %g' % (epoch, j, num_batches, batch_loss), end=\"\\r\", flush=True)\n",
    "            if j % draw_step == 0:\n",
    "                draw_conv_filters(sess, weights[0], epoch, j, \"conv1\", SAVE_DIR)\n",
    "\n",
    "        print(\"\", flush=True)\n",
    "        valid_acc = accuracy.eval(feed_dict={x: valid_x, y_: valid_y})\n",
    "        valid_loss = loss.eval(feed_dict={x: valid_x, y_: valid_y})\n",
    "        print('epoch %d, valid loss %g, valid accuracy %g' % (epoch, valid_loss, valid_acc))\n",
    "\n",
    "    test_acc = accuracy.eval(feed_dict={x: test_x, y_: test_y})\n",
    "    test_loss = loss.eval(feed_dict={x: test_x, y_: test_y})\n",
    "    print('test loss %g, test accuracy %g' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
